version: '3.8'

# =============================================================================
# Biya Indexer All-in-One 部署配置
# 
# 本配置文件整合了所有中间件服务，适用于单节点部署场景。
# 
# 使用方式:
#   启动: docker-compose -f docker-compose.all-in-one.yaml up -d
#   停止: docker-compose -f docker-compose.all-in-one.yaml down
#   日志: docker-compose -f docker-compose.all-in-one.yaml logs -f
# =============================================================================

networks:
  biya-net:
    name: biya-net
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/16

services:
  # ===========================================================================
  # Dragonfly - 高性能 Redis 兼容缓存
  # ===========================================================================
  dragonfly:
    image: docker.dragonflydb.io/dragonflydb/dragonfly:${DRAGONFLY_VERSION:-latest}
    container_name: dragonfly
    hostname: dragonfly
    networks:
      - biya-net
    ports:
      - "${DRAGONFLY_PORT:-6379}:6379"
      - "${DRAGONFLY_ADMIN_PORT:-6380}:6380"
    volumes:
      - dragonfly-data:/data
      - ./logs/dragonfly:/logs
    environment:
      - DRAGONFLY_LOGTOSTDERR=false
      - DRAGONFLY_ALSOLOGTOSTDERR=false
      - DRAGONFLY_MINLOGLEVEL=1
    command: >
      --logtostderr=false
      --alsologtostderr=false
      --minloglevel=1
      --proactor_threads ${DRAGONFLY_THREADS:-4}
      --maxmemory ${DRAGONFLY_MEMORY:-2gb}
      --cache_mode=true
      --dbnum 16
      --dir /data
      --dbfilename dump
      --nodf_snapshot_format
    ulimits:
      memlock: -1
      nofile: 65535
    shm_size: 512mb
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "-p", "6379", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"

  # ===========================================================================
  # Zookeeper - Kafka 依赖的协调服务
  # ===========================================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:${ZOOKEEPER_VERSION:-7.5.0}
    container_name: zookeeper
    hostname: zookeeper
    networks:
      - biya-net
    ports:
      - "${ZOOKEEPER_PORT:-2181}:2181"
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_INIT_LIMIT: 5
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================================================
  # Kafka - 消息队列
  # ===========================================================================
  kafka:
    image: confluentinc/cp-kafka:${KAFKA_VERSION:-7.5.0}
    container_name: kafka
    hostname: kafka
    networks:
      - biya-net
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "${KAFKA_PORT:-9092}:9092"
      - "${KAFKA_INTERNAL_PORT:-29092}:29092"
    volumes:
      - kafka-data:/var/lib/kafka/data
    environment:
      # Broker 配置
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      
      # 监听器配置
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:29092
      
      # Topic 配置
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: ${KAFKA_AUTO_CREATE_TOPICS:-true}
      KAFKA_NUM_PARTITIONS: ${KAFKA_NUM_PARTITIONS:-3}
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      
      # 日志保留
      KAFKA_LOG_RETENTION_HOURS: ${KAFKA_LOG_RETENTION_HOURS:-168}
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      
      # 性能调优
      KAFKA_COMPRESSION_TYPE: producer
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================================================
  # Kafka UI - Kafka Web 管理界面（可选）
  # ===========================================================================
  kafka-ui:
    image: provectuslabs/kafka-ui:${KAFKA_UI_VERSION:-latest}
    container_name: kafka-ui
    hostname: kafka-ui
    networks:
      - biya-net
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "${KAFKA_UI_PORT:-8080}:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: biya-kafka
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    restart: unless-stopped
    profiles:
      - ui
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================================================
  # ScyllaDB - 高性能 NoSQL 数据库
  # ===========================================================================
  scylla:
    image: scylladb/scylla:5.2
    container_name: scylla
    user: "0:0" # Run as root to avoid permission issues
    networks:
      - biya-net
    ports:
      - "${SCYLLA_CQL_PORT:-9042}:9042"
    volumes:
      - scylla-data:/var/lib/scylla
    command: >
      --smp 1 --memory 1G --overprovisioned 1 --developer-mode 1 --api-address 0.0.0.0
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "nodetool status || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ===========================================================================
  # ScyllaDB Initialization - 等待 ScyllaDB 就绪
  # ===========================================================================
  scylla-init:
    image: scylladb/scylla:5.2
    container_name: scylla-init
    networks:
      - biya-net
    depends_on:
      scylla:
        condition: service_healthy
    volumes:
      - ./scripts/init-scylladb.sh:/init-scylladb.sh:ro
    entrypoint: [ "/bin/bash", "/init-scylladb.sh" ]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    restart: "no"

  # ===========================================================================
  # Indexer Client - 从链上获取数据并写入 Kafka
  # ===========================================================================
  indexer-client:
    image: indexer-client:${INDEXER_VERSION:-latest}
    container_name: indexer-client
    hostname: indexer-client
    networks:
      - biya-net
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - GRPC_STREAM_ENDPOINT=${INDEXER_CHAIN_GRPC_STREAM:-http://host.docker.internal:9999}
      - GRPC_QUERY_ENDPOINT=${INDEXER_CHAIN_GRPC_QUERY:-http://host.docker.internal:9900}
      - KAFKA_BROKERS=kafka:29092
      - KAFKA_TOPIC=${KAFKA_TOPIC:-biya-events}
      - KAFKA_CLIENT_ID=${KAFKA_CLIENT_ID:-biya-indexer-client}
      - RUST_LOG=${LOG_LEVEL:-info}
      - HOST_RPC_ADDR=${HOST_LAN_IP:-host.docker.internal}
    volumes:
      - ./logs/indexer-client:/logs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"

  # ===========================================================================
  # Indexer Consumer - 从 Kafka 消费数据并写入 ScyllaDB 和 Dragonfly
  # ===========================================================================
  indexer-consumer:
    image: indexer-consumer:${INDEXER_VERSION:-latest}
    container_name: indexer-consumer
    hostname: indexer-consumer
    networks:
      - biya-net
    depends_on:
      kafka:
        condition: service_healthy
      dragonfly:
        condition: service_healthy
      scylla-init:
        condition: service_completed_successfully
    environment:
      - RUST_LOG=${LOG_LEVEL:-info}
      - KAFKA_BROKERS=kafka:29092
      - KAFKA_TOPIC=${KAFKA_TOPIC:-biya-events}
      - KAFKA_CONSUMER_GROUP=${KAFKA_CONSUMER_GROUP:-biya-consumers}
      - KAFKA_REDIS_CONSUMER_GROUP=${KAFKA_REDIS_CONSUMER_GROUP:-biya-consumers-redis}
      - KAFKA_SCYLLADB_CONSUMER_GROUP=${KAFKA_SCYLLADB_CONSUMER_GROUP:-biya-consumers-scylladb}
      - REDIS_URL=redis://dragonfly:6379
      - SCYLLADB_NODES=scylla:9042
      - GRPC_STREAM_ENDPOINT=${INDEXER_CHAIN_GRPC_STREAM:-http://host.docker.internal:9999}
      - GRPC_QUERY_ENDPOINT=${INDEXER_CHAIN_GRPC_QUERY:-http://host.docker.internal:9900}
    volumes:
      - ./logs/indexer-consumer:/logs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"

  # ===========================================================================
  # Indexer gRPC Server - 对外提供 gRPC 查询服务
  # ===========================================================================
  indexer-grpc-server:
    image: indexer-server:${INDEXER_VERSION:-latest}
    container_name: indexer-grpc-server
    hostname: indexer-grpc-server
    networks:
      - biya-net
    depends_on:
      dragonfly:
        condition: service_healthy
      scylla:
        condition: service_healthy
      scylla-init:
        condition: service_completed_successfully
    ports:
      - "${INDEXER_GRPC_PORT:-50052}:50052"
      - "${INDEXER_GRPC_WEB_PORT:-50053}:50053"
    environment:
      - GRPC_LISTEN_ADDR=0.0.0.0:50052
      - GRPC_WEB_LISTEN_ADDR=0.0.0.0:50053
      - GRPC_WEB_ALLOWED_ORIGINS=${GRPC_WEB_ALLOWED_ORIGINS:-*}
      - REDIS_URL=redis://dragonfly:6379
      - SCYLLA_NODES=scylla:9042
      - CHAIN_GRPC_ENDPOINT=${INDEXER_CHAIN_GRPC:-http://host.docker.internal:9900}
      - TENDERMINT_RPC_ENDPOINT=${INDEXER_CHAIN_RPC:-http://host.docker.internal:26657}
      - CHAIN_ID=${CHAIN_ID:-biya-1}
      - FEE_PAYER_ADDRESS=${FEE_PAYER_ADDRESS:-}
      - FEE_PAYER_PRIVATE_KEY=${FEE_PAYER_PRIVATE_KEY:-}
      - RUST_LOG=${LOG_LEVEL:-info}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"

# =============================================================================
# 数据卷定义
# =============================================================================
volumes:
  # Dragonfly 数据
  dragonfly-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/dragonfly

  # Zookeeper 数据
  zookeeper-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/zookeeper/data
  zookeeper-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/zookeeper/logs

  # Kafka 数据
  kafka-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/kafka

  # ScyllaDB 数据
  scylla-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/scylla/data
  scylla-config:
    driver: local

